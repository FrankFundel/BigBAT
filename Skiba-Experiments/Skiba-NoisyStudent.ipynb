{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90944339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-12 17:43:28.905686: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-12 17:43:29.311717: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-12 17:43:32.207423: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/ffundel/.local/lib:\n",
      "2023-01-12 17:43:32.207749: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/ffundel/.local/lib:\n",
      "2023-01-12 17:43:32.207777: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.utils import shuffle\n",
    "import cv2\n",
    "import torch\n",
    "#import torchaudio\n",
    "\n",
    "from tools import prepare, mixup, preprocess, noise, getCorrects\n",
    "\n",
    "sys.path.append('../BAT/datasets/')\n",
    "\n",
    "classes = {\n",
    "    \"Rhinolophus ferrumequinum\": 0,\n",
    "    \"Rhinolophus hipposideros\": 1,\n",
    "    \"Myotis daubentonii\": 2,\n",
    "    \"Myotis brandtii\": 3,\n",
    "    \"Myotis mystacinus\": 4,\n",
    "    \"Myotis emarginatus\": 5,\n",
    "    \"Myotis nattereri\": 6,\n",
    "    #\"Myotis bechsteinii\": 7,\n",
    "    \"Myotis myotis\": 7,\n",
    "    \"Myotis dasycneme\": 8,\n",
    "    \"Nyctalus noctula\": 9,\n",
    "    \"Nyctalus leisleri\": 10,\n",
    "    \"Pipistrellus pipistrellus\": 11,\n",
    "    \"Pipistrellus nathusii\": 12,\n",
    "    \"Pipistrellus kuhlii\": 13,\n",
    "    \"Eptesicus serotinus\": 14,\n",
    "    \"Eptesicus nilssonii\": 15,\n",
    "    #\"Plecotus auritus\": 16,\n",
    "    #\"Plecotus austriacus\": 16,\n",
    "    #\"Barbastella barbastellus\": 16,\n",
    "    #\"Tadarida teniotis\": 16,\n",
    "    \"Miniopterus schreibersii\": 16,\n",
    "    #\"Hypsugo savii\": 18,\n",
    "    \"Vespertilio murinus\": 17,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49421f02",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b04dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfft = 512\n",
    "num_bands = nfft // 2 + 1\n",
    "\n",
    "max_len = 60\n",
    "patch_len = 44\n",
    "patch_skip = 22\n",
    "\n",
    "samples_per_step = patch_skip * (nfft // 4)\n",
    "seq_len = (max_len + 1) * samples_per_step\n",
    "seq_skip = (max_len + 1) * samples_per_step // 4\n",
    "    \n",
    "data_path = \"../BAT/datasets/prepared_signal.h5\"\n",
    "X_train, Y_train, X_test, Y_test, X_val, Y_val = prepare(data_path, classes, seq_len, seq_skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1883c89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total sequences:\", len(X_train) + len(X_test) + len(X_val))\n",
    "print(\"Train sequences:\", X_train.shape, Y_train.shape)\n",
    "print(\"Test sequences:\", X_test.shape, Y_test.shape)\n",
    "print(\"Validation sequences:\", X_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf07ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holdout for unsupervised training\n",
    "holdout = True\n",
    "h_split = 0.9\n",
    "h_train = int(len(X_train) * h_split)\n",
    "if holdout:\n",
    "    X_unlabeled = X_train[:h_train]\n",
    "    Y_unlabeled = Y_train[:h_train]\n",
    "    \n",
    "    X_train = X_train[h_train:]\n",
    "    Y_train = Y_train[h_train:]\n",
    "    \n",
    "    print(\"Total sequences:\", len(X_train) + len(X_test) + len(X_val))\n",
    "    print(\"Train sequences:\", X_train.shape, Y_train.shape)\n",
    "    print(\"Test sequences:\", X_test.shape, Y_test.shape)\n",
    "    print(\"Validation sequences:\", X_val.shape, Y_val.shape)\n",
    "    print(\"Total unlabeled sequences:\", len(X_unlabeled))\n",
    "    print(\"Unlabeled sequences:\", X_unlabeled.shape)\n",
    "else:\n",
    "    X_unlabeled = X_train[:h_train]\n",
    "    Y_unlabeled = Y_train[:h_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a791433",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583d441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torchsummary import summary\n",
    "from torchmetrics.functional import f1_score\n",
    "\n",
    "from SAM import SAM\n",
    "from ASL import AsymmetricLoss\n",
    "from BigBAT import BigBAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73946b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model, device_ids=[0, 1])\n",
    "    model_student = nn.DataParallel(model_student, device_ids=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3294d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "patch_embedding = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=(3, 5), stride=(2, 3), padding=3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=(3, 5), stride=(2, 3), padding=3),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "    \n",
    "            nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 3), padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 3), padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "    \n",
    "            nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "        ).to(device)\n",
    "\n",
    "patch_embedding2 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=(3, 5), stride=(2, 3), padding=3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=(3, 5), stride=(2, 3), padding=3),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "    \n",
    "            nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 3), padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 3), padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "    \n",
    "            nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "        ).to(device)\n",
    "\n",
    "summary(patch_embedding, (1, 44, 257))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635929c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 23\n",
    "lr = 0.0005\n",
    "warmup_epochs = 5\n",
    "d_model = 64\n",
    "\n",
    "nhead = 2\n",
    "dim_feedforward = 32\n",
    "num_layers = 2\n",
    "dropout = 0.3\n",
    "classifier_dropout = 0.3\n",
    "num_classes = len(list(classes))\n",
    "    \n",
    "model = BigBAT(\n",
    "    max_len=max_len,\n",
    "    patch_len=patch_len,\n",
    "    patch_skip=patch_skip,\n",
    "    d_model=d_model,\n",
    "    num_classes=len(list(classes)),\n",
    "    patch_embedding=patch_embedding,\n",
    "    use_cls=False,\n",
    "    nhead=nhead,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout,\n",
    "    classifier_dropout=classifier_dropout,\n",
    ")\n",
    "model_student = BigBAT(\n",
    "    max_len=max_len,\n",
    "    patch_len=patch_len,\n",
    "    patch_skip=patch_skip,\n",
    "    d_model=d_model,\n",
    "    num_classes=len(list(classes)),\n",
    "    patch_embedding=patch_embedding2,\n",
    "    use_cls=False,\n",
    "    nhead=nhead,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout,\n",
    "    classifier_dropout=classifier_dropout,\n",
    ")\n",
    "    \n",
    "model.to(device)\n",
    "model_student.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a5e13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#freq_masking = torchaudio.transforms.FrequencyMasking(freq_mask_param=50)\n",
    "#time_masking = torchaudio.transforms.TimeMasking(time_mask_param=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1529c388",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = batch_size * int(len(X_train) / batch_size)\n",
    "test_len = batch_size * int(len(X_test) / batch_size)\n",
    "val_len = batch_size * int(len(X_val) / batch_size)\n",
    "\n",
    "train_data = TensorDataset(X_train[:train_len], Y_train[:train_len])\n",
    "test_data = TensorDataset(X_test[:test_len], Y_test[:test_len])\n",
    "val_data = TensorDataset(X_val[:val_len], Y_val[:val_len])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "unlabeled_len = batch_size * int(len(X_unlabeled) / batch_size)\n",
    "unlabeled_data = TensorDataset(X_unlabeled[:unlabeled_len], Y_unlabeled[:unlabeled_len])\n",
    "unlabeled_loader = DataLoader(unlabeled_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8dc3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sequence(X, Y, k):\n",
    "    plt.figure(figsize = (20, 2.5))\n",
    "    x = X[k].cpu().detach().numpy()\n",
    "    y = Y[k].cpu().detach().numpy()\n",
    "    plt.imshow(np.rot90(x), interpolation='nearest', aspect='auto', cmap='inferno')\n",
    "    plt.colorbar()\n",
    "    label_list = []\n",
    "    if(len(y.shape) > 0):\n",
    "        for i in np.argwhere(y == 1)[:,0]:\n",
    "            label_list.append(list(classes)[i])\n",
    "        plt.title(\", \".join(label_list))\n",
    "    else:\n",
    "        plt.title(list(classes)[y])\n",
    "    print(x.shape)\n",
    "\n",
    "k = np.random.randint(0, batch_size)\n",
    "X1, Y1 = next(iter(train_loader))\n",
    "X1, Y1 = X1.cuda(), Y1.cuda()\n",
    "X1, Y1 = mixup(X1, Y1, min_seq=1, max_seq=3)\n",
    "X1 = preprocess(X1)\n",
    "X1 = noise(X1)\n",
    "#X1 = freq_masking(X1)\n",
    "#X1 = time_masking(X1)\n",
    "        \n",
    "plot_sequence(X1, Y1, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e7e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = AsymmetricLoss(gamma_neg=2, gamma_pos=1, clip=0)\n",
    "\n",
    "#base_optimizer = torch.optim.SGD\n",
    "#optimizer = SAM(model.parameters(), base_optimizer, lr=lr, momentum=0.9)\n",
    "\n",
    "optimizer = torch.optim.SGD(model_student.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4, nesterov=True, dampening=0)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer=optimizer, T_0=warmup_epochs)\n",
    "\n",
    "min_val_loss = np.inf\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556673e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothen(y, num_classes, l=.02):\n",
    "    return y * (1.0 - l) + (y.sum(dim=1, keepdim=True) * l / num_classes).repeat(1, num_classes)\n",
    "\n",
    "def getSpeciesMasks(species):\n",
    "    masks = torch.eye(len(species))\n",
    "    for i, s in enumerate(species):\n",
    "        genus = s.split(' ')[0]\n",
    "        for k, ss in enumerate(species):\n",
    "            if ss.startswith(genus):\n",
    "                masks[i, k] = 1\n",
    "        masks[i, i] = 0\n",
    "    return masks\n",
    "\n",
    "def speciesSmoothing(Y, masks, l=.1):\n",
    "    ny = torch.zeros_like(Y)\n",
    "    for i, y in enumerate(Y):\n",
    "        lbl = torch.nonzero(y)\n",
    "        ny[i] = y * (1.0 - l) + masks[lbl].sum(0) * (y.sum() * l / max(masks[lbl].sum(), 1))\n",
    "    return ny\n",
    "\n",
    "test_y = torch.zeros(3, num_classes).to(device)\n",
    "test_y[0, 4] = 1\n",
    "test_y[0, 5] = 1\n",
    "test_y[1, 11] = 1\n",
    "test_y[2, 4] = 1\n",
    "test_y[2, 11] = 1\n",
    "test_y[2, 17] = 1\n",
    "print(test_y)\n",
    "masks = getSpeciesMasks(list(classes)).to(device)\n",
    "test_smooth = smoothen(test_y, num_classes)\n",
    "test_specsmooth = speciesSmoothing(test_y, masks)\n",
    "print(test_smooth, test_smooth.sum())\n",
    "print(test_specsmooth, test_specsmooth.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3876515",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('BigBAT-trained.pth'))\n",
    "model_student.load_state_dict(torch.load('BigBAT-trained.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34897dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from contrastive_learner.contrastive_learner import RandomApply\n",
    "\n",
    "augment = nn.Sequential(\n",
    "    #RandomApply(T.ColorJitter(0.8, 0.8, 0.8, 0.2), p=0.8),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomVerticalFlip(),\n",
    "    RandomApply(T.GaussianBlur((23, 23), (1.5, 1.5)), p=0.3),\n",
    "    RandomApply(noise, p=0.5),\n",
    "    T.RandomErasing()\n",
    "    #T.RandomResizedCrop((1343, 257))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0630d19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_u = 2.0\n",
    "\n",
    "def train_epoch(model, model_student, epoch, criterion, optimizer, scheduler, dataloader, dataloader_u, device):\n",
    "    model_student.train()\n",
    "    model.eval()\n",
    "        \n",
    "    num_batches = len(dataloader_u)\n",
    "    num_samples = 0\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    iter_u = iter(dataloader_u)\n",
    "        \n",
    "    # Normal training procedure\n",
    "    for batch, (inputs, labels) in enumerate(tqdm(dataloader)):\n",
    "        try:\n",
    "            inputs_u, _ = next(iter_u)\n",
    "        except StopIteration:\n",
    "            iter_u = iter(dataloader_u)\n",
    "            inputs_u, _ = next(iter_u)\n",
    "        \n",
    "        inputs_u = inputs_u.to(device)\n",
    "        inputs_u = preprocess(inputs_u)\n",
    "        \n",
    "        # Forward Pass to get the pseudo labels\n",
    "        with torch.no_grad():\n",
    "            outputs_pseudo = model(inputs_u).detach()\n",
    "            outputs_pseudo = torch.sigmoid(outputs_pseudo)\n",
    "            max_probs, pseudo_labels = torch.max(outputs_pseudo, 1)\n",
    "            pseudo_labels = torch.nn.functional.one_hot(pseudo_labels, num_classes)\n",
    "            #unlabeled_loss = alpha_weight(step) * criterion(outputs_pseudo, pseudo_labels)\n",
    "            #unlabeled_loss = criterion(outputs_pseudo, pseudo_labels)\n",
    "            \n",
    "            # filter images that the teacher has low confidences on\n",
    "            mask = max_probs.ge(0.6)\n",
    "            pseudo_labels = pseudo_labels[mask]\n",
    "        \n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        inputs, labels = mixup(inputs, labels, min_seq=1, max_seq=3)\n",
    "        inputs = preprocess(inputs)\n",
    "\n",
    "        # Augment inputs_u and inputs\n",
    "        inputs = augment(inputs)\n",
    "        inputs_u = augment(inputs_u)\n",
    "        \n",
    "        outputs_u = model_student(inputs_u[mask])\n",
    "        unlabeled_loss = criterion(outputs_u, pseudo_labels)\n",
    "        \n",
    "        outputs = model_student(inputs)\n",
    "        labeled_loss = criterion(outputs, labels) # reduction : mean\n",
    "        loss = labeled_loss + lambda_u * unlabeled_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #optimizer.first_step()\n",
    "        #loss = criterion(outputs, labels) + alpha_weight(step) * criterion(outputs_unlabeled, pseudo_labels)\n",
    "        #loss.backward()\n",
    "        #optimizer.second_step()\n",
    "\n",
    "        # Calculate Loss\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += getCorrects(outputs_u, pseudo_labels)\n",
    "        num_samples += inputs.size(0)\n",
    "    \n",
    "        # Perform learning rate step\n",
    "        #scheduler.step()\n",
    "        scheduler.step(epoch + batch / num_batches)\n",
    "            \n",
    "    epoch_loss = running_loss / num_samples\n",
    "    epoch_acc = running_corrects / num_samples\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18c274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model, epoch, criterion, optimizer, dataloader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    num_batches = len(dataloader)\n",
    "    num_samples = len(dataloader.dataset)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for batch, (inputs, labels) in enumerate(tqdm(dataloader)):\n",
    "            # Transfer Data to GPU if available\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            inputs, labels = mixup(inputs, labels, min_seq=1, max_seq=3)\n",
    "            inputs = preprocess(inputs)\n",
    "\n",
    "            # Clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward Pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute Loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Calculate Loss\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += getCorrects(outputs, labels)\n",
    "\n",
    "        epoch_loss = running_loss / num_samples\n",
    "        epoch_acc = running_corrects / num_samples\n",
    "    \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95cc47b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ac1d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb_config = {\n",
    "    \"epochs\": epochs,\n",
    "    \"lr\": lr,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"warmup_epochs\": warmup_epochs,\n",
    "    \"d_model\": d_model,\n",
    "    \"nhead\": nhead,\n",
    "    \"dim_feedforward\": dim_feedforward,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"dropout\": dropout,\n",
    "    \"classifier_dropout\": classifier_dropout\n",
    "}\n",
    "\n",
    "wandb.init(project=\"BigBAT\", entity=\"frankfundel\", config=wandb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2380367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 5\n",
    "for i in range(iterations):\n",
    "    for epoch in range(epochs):\n",
    "        end = time.time()\n",
    "        print(f\"==================== Starting at epoch {epoch} ====================\", flush=True)\n",
    "\n",
    "        train_loss, train_acc = train_epoch(model, model_student, epoch, criterion, optimizer, scheduler, train_loader,\n",
    "                                            unlabeled_loader, device)\n",
    "        print('Training loss: {:.4f} Acc: {:.4f}'.format(train_loss, train_acc), flush=True)\n",
    "\n",
    "        #val_loss, val_acc = test_epoch(model, epoch, criterion, optimizer, val_loader, device)\n",
    "        #print('Validation loss: {:.4f} Acc: {:.4f}'.format(val_loss, val_acc), flush=True)\n",
    "        \n",
    "        val_loss, val_acc = test_epoch(model_student, epoch, criterion, optimizer, val_loader, device)\n",
    "        print('Validation loss student: {:.4f} Acc: {:.4f}'.format(val_loss, val_acc), flush=True)\n",
    "\n",
    "        wandb.log({\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_acc\": val_acc,\n",
    "        })\n",
    "\n",
    "        if min_val_loss > val_loss:\n",
    "            print('val_loss decreased, saving model', flush=True)\n",
    "            min_val_loss = val_loss\n",
    "\n",
    "            # Saving State Dict\n",
    "            torch.save(model.state_dict(), 'BigBAT-NoisyStudent.pth')\n",
    "\n",
    "    if i != iterations - 1:\n",
    "        print(\"student becomes the new teacher\")\n",
    "        model = model_student\n",
    "\n",
    "        # create new student model, optimizer, scheduler etc. / change batch sizes accordingly depending on GPU Memory\n",
    "        #batch_size = batch_sizes[i+2]\n",
    "        model_student = BigBAT(\n",
    "            max_len=max_len,\n",
    "            patch_len=patch_len,\n",
    "            patch_skip=patch_skip,\n",
    "            d_model=d_model,\n",
    "            num_classes=len(list(classes)),\n",
    "            patch_embedding=nn.Sequential(\n",
    "                nn.Conv2d(1, 16, kernel_size=(3, 5), stride=(2, 3), padding=3),\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Conv2d(16, 32, kernel_size=(3, 5), stride=(2, 3), padding=3),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "                nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 3), padding=1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 3), padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(),\n",
    "            ),\n",
    "            use_cls=False,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            classifier_dropout=classifier_dropout,\n",
    "        )\n",
    "        model_student = model_student.to(device)\n",
    "        model_student.load_state_dict(torch.load('BigBAT-trained.pth'))\n",
    "        if device == 'cuda':\n",
    "            model_student = torch.nn.DataParallel(model_student) \n",
    "        optimizer = torch.optim.SGD(model_student.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4, nesterov=True, dampening=0)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer=optimizer, T_0=warmup_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fad11ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load after training\n",
    "model.load_state_dict(torch.load('BigBAT-NoisyStudent.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15f57fc",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bff82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_corrects = 0.0\n",
    "repeats = 5\n",
    "\n",
    "predictions = []\n",
    "targets = []\n",
    "\n",
    "for r in range(repeats):\n",
    "    # iterate over test data\n",
    "    for inputs, labels in tqdm(test_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        inputs, labels = mixup(inputs, labels, min_seq=1, max_seq=3)\n",
    "        inputs = preprocess(inputs)\n",
    "\n",
    "        output = model(inputs) # Feed Network\n",
    "        predictions.extend(output.data.cpu().numpy())\n",
    "        targets.extend(labels.data.cpu().numpy())\n",
    "        mixed_corrects += getCorrects(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1604fc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "mixed_test_acc = mixed_corrects / (repeats * len(test_data))\n",
    "mixed_f1_micro = f1_score(sigmoid(np.asarray(predictions)) > 0.5, np.asarray(targets), average='micro')\n",
    "mixed_f1_macro = f1_score(sigmoid(np.asarray(predictions)) > 0.5, np.asarray(targets), average='macro')\n",
    "\n",
    "print(\"Mixed test acc:\", mixed_test_acc)\n",
    "print(\"Mixed f1 micro:\", mixed_f1_micro)\n",
    "print(\"Mixed f1 macro:\", mixed_f1_macro)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params, \"params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593ab0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "Y_pred = []\n",
    "Y_true = []\n",
    "corrects = 0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# iterate over test data\n",
    "for inputs, labels in tqdm(test_loader):\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    inputs = preprocess(inputs)\n",
    "    \n",
    "    output = model(inputs) # Feed Network\n",
    "\n",
    "    output = torch.argmax(output, 1).data.cpu().numpy()\n",
    "    Y_pred.extend(output) # Save Prediction\n",
    "\n",
    "    labels = torch.argmax(labels, 1).data.cpu().numpy()\n",
    "    #labels = labels.data.cpu().numpy()\n",
    "    Y_true.extend(labels) # Save Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e11e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build confusion matrix\n",
    "cf_matrix = confusion_matrix(Y_true, Y_pred, normalize=\"all\") # normalize{‘true’, ‘pred’, ‘all’}, default=None\n",
    "df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=-1), index = [i for i in classes], columns = [i for i in classes])\n",
    "plt.figure(figsize = (12, 7))\n",
    "s = sn.heatmap(df_cm, annot=True, fmt='.2f')\n",
    "s.set(xlabel='True', ylabel='Pred')\n",
    "plt.savefig('BigBAT-NoisyStudent.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f251ae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrects = np.equal(Y_pred, Y_true).sum()\n",
    "single_test_acc = corrects / len(Y_pred)\n",
    "single_f1 = f1_score(Y_true, Y_pred, average=None).mean()\n",
    "\n",
    "print(\"Single test accuracy:\", single_test_acc)\n",
    "print(\"Single F1-score:\", single_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c63a399",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\n",
    "    \"mixed_test_acc\": mixed_test_acc,\n",
    "    \"mixed_f1_micro\": mixed_f1_micro,\n",
    "    \"mixed_f1_macro\": mixed_f1_macro,\n",
    "    \"single_test_acc\": single_test_acc,\n",
    "    \"single_f1_micro\": single_f1,\n",
    "    \"num_params\": pytorch_total_params,\n",
    "})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d953a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
